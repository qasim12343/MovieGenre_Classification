{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10061] No\n",
      "[nltk_data]     connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     No connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import copy\n",
    "# downlaod some necessary data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.txt',delimiter=' ::: ', engine='python')\n",
    "test_data = pd.read_csv('test.txt', delimiter= ' ::: ', engine= 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing train data and\n",
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "preprocess_data = copy.deepcopy(train_data)\n",
    "preprocess_data['description'] = preprocess_data['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eighteen yearold alex 13 yearold maggie four year old kay happy mothers new union nameless violent bluecollar drunk agree try make work suddenly killed car crash alex forced mature quickly look sisters promised looking move better life drops college works fulltime grocery store making minimum wage meanwhile barrier stepfathers bitter resentment grows antagonistic maggie verge womanhood kay beginning learn ways world alex grows increasingly protective sisters innocence support system girlfriend courtney threat prison abandoning sisters keeping alex violence far willing go keep family together\n",
      "eighteen yearold alex 13 yearold maggie four year old kay happy mothers new union nameless violent bluecollar drunk agree try make work suddenly killed car crash alex forced mature quickly look sisters promised looking move better life drops college works fulltime grocery store making minimum wage meanwhile barrier stepfathers bitter resentment grows antagonistic maggie verge womanhood kay beginning learn ways world alex grows increasingly protective sisters innocence support system girlfriend courtney threat prison abandoning sisters keeping alex violence far willing go keep family together\n"
     ]
    }
   ],
   "source": [
    "print(train_data['description'][1])\n",
    "print(preprocess_data['description'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(text):\n",
    "    tf_text = Counter(text.split())\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i] / float(len(text.split()))\n",
    "    return tf_text\n",
    "\n",
    "def compute_idf(word, corpus):\n",
    "    n_documents_with_word = sum([1 for document in corpus if word in document])\n",
    "    if n_documents_with_word == 0:\n",
    "        return 0\n",
    "    return np.log10(len(corpus) / n_documents_with_word)\n",
    "\n",
    "corpus = data['description'].tolist()\n",
    "tfidf_matrix = []\n",
    "\n",
    "for text in corpus:\n",
    "    tfidf_text = []\n",
    "    computed_tf = compute_tf(text)\n",
    "    for word in text.split():\n",
    "        tfidf = computed_tf[word] * compute_idf(word, corpus)\n",
    "        tfidf_text.append(tfidf)\n",
    "    tfidf_matrix.append(tfidf_text)\n",
    "\n",
    "max_len = max(len(row) for row in tfidf_matrix)\n",
    "tfidf_matrix = [row + [0] * (max_len - len(row)) for row in tfidf_matrix]\n",
    "tfidf_matrix = np.array(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, x_test, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], x_test)\n",
    "        distances.append((dist, y_train.iloc[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    neighbors = distances[:k]\n",
    "    output_values = [neighbor[1] for neighbor in neighbors]\n",
    "    prediction = Counter(output_values).most_common(1)[0][0]\n",
    "    return prediction\n",
    "\n",
    "# تقسیم داده‌ها به آموزشی و ارزیابی\n",
    "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    X_train = X[train_indices]\n",
    "    X_val = X[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_val = y.iloc[test_indices]\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf_matrix, data['genre'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comedy': {'precision': 0.14285714285714285, 'recall': 0.10344827586206896, 'f1-score': 0.12000000000000001}, 'family': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'romance': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'animation': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'adventure': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'sci-fi': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'biography': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'crime': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'documentary': {'precision': 0.2532051282051282, 'recall': 0.32916666666666666, 'f1-score': 0.286231884057971}, 'reality-tv': {'precision': 0.5, 'recall': 0.0625, 'f1-score': 0.1111111111111111}, 'horror': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'drama': {'precision': 0.25635103926096997, 'recall': 0.4475806451612903, 'f1-score': 0.32599118942731276}, 'fantasy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'music': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'musical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'western': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'history': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666}, 'talk-show': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'sport': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'thriller': {'precision': 0.05, 'recall': 0.029411764705882353, 'f1-score': 0.03703703703703704}, 'mystery': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0}, 'action': {'precision': 0.05555555555555555, 'recall': 0.038461538461538464, 'f1-score': 0.04545454545454546}, 'short': {'precision': 0.09375, 'recall': 0.05555555555555555, 'f1-score': 0.06976744186046512}}\n"
     ]
    }
   ],
   "source": [
    "def classification_report(y_true, y_pred):\n",
    "    labels = list(set(y_true))\n",
    "    report = {}\n",
    "    for label in labels:\n",
    "        true_positive = sum((y_true == label) & (y_pred == label))\n",
    "        false_positive = sum((y_true != label) & (y_pred == label))\n",
    "        false_negative = sum((y_true == label) & (y_pred != label))\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        report[label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1-score': f1_score\n",
    "        }\n",
    "    return report\n",
    "\n",
    "y_pred = []\n",
    "for i in range(len(X_val)):\n",
    "    y_pred.append(knn_predict(X_train, y_train, X_val[i]))\n",
    "\n",
    "print(classification_report(np.array(y_val), np.array(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Genre: documentary\n",
      "horror\n"
     ]
    }
   ],
   "source": [
    "def preprocess_and_vectorize(description, corpus):\n",
    "    description = preprocess_text(description)\n",
    "    tfidf_text = []\n",
    "    computed_tf = compute_tf(description)\n",
    "    for word in description.split():\n",
    "        tfidf = computed_tf[word] * compute_idf(word, corpus)\n",
    "        tfidf_text.append(tfidf)\n",
    "    if len(tfidf_text) < max_len:\n",
    "        tfidf_text.extend([0] * (max_len - len(tfidf_text)))\n",
    "    return np.array(tfidf_text)\n",
    "\n",
    "def predict_genre(description, X_train, y_train, corpus):\n",
    "    description_vector = preprocess_and_vectorize(description, corpus)\n",
    "    predicted_genre = knn_predict(X_train, y_train, description_vector, k=5)\n",
    "    return predicted_genre\n",
    "\n",
    "# نمونه‌ای از توضیحات جدید برای آزمایش\n",
    "new_description = train_data['description'][2]\n",
    "predicted_genre = predict_genre(new_description, X_train, y_train, corpus)\n",
    "print(f\"Predicted Genre: {predicted_genre}\")\n",
    "print(train_data['genre'][2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qasem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qasem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Qasem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import copy\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.txt', delimiter=' ::: ', engine='python')\n",
    "test_data = pd.read_csv('test.txt', delimiter=' ::: ', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing train data and\n",
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Preprocess data\n",
    "\n",
    "preprocess_data = copy.deepcopy(train_data.head(1000))\n",
    "preprocess_data['description'] = preprocess_data['description'].apply(preprocess_text)\n",
    "data = preprocess_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# X_train = copy.deepcopy(data['description'])\n",
    "# y_train = copy.deepcopy(data['genre'])\n",
    "\n",
    "# # تبدیل داده‌ها به فرمت مناسب\n",
    "# X_train = X_train.values.reshape(-1, 1)\n",
    "\n",
    "# # ایجاد نمونه‌های کاهشی\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# # تبدیل داده‌های کاهشی به DataFrame\n",
    "# resampled_train_data = pd.DataFrame({\n",
    "#     'description': X_resampled.flatten(),\n",
    "#     'genre': y_resampled\n",
    "# })\n",
    "\n",
    "# des = []\n",
    "# genre = []\n",
    "# for i in range(len(X_resampled)):\n",
    "#     des.append(X_resampled[i][0])\n",
    "#     genre.append(y_resampled[i][0])\n",
    "\n",
    "# n = pd.DataFrame({'description':des, 'genre':y_resampled})\n",
    "# data = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precompute IDF values\n",
    "def compute_idf(corpus):\n",
    "    idf_dict = defaultdict(int)\n",
    "    for document in corpus:\n",
    "        unique_words = set(document.split())\n",
    "        for word in unique_words:\n",
    "            idf_dict[word] += 1\n",
    "    total_documents = len(corpus)\n",
    "    idf_dict = {word: np.log10(total_documents / count) for word, count in idf_dict.items()}\n",
    "    return idf_dict\n",
    "\n",
    "# Compute TF-IDF\n",
    "corpus = data['description'].tolist()\n",
    "\n",
    "idf_dict = compute_idf(corpus)\n",
    "\n",
    "def compute_tf(text):\n",
    "    tokens = text.split()\n",
    "    tf_text = Counter(tokens)\n",
    "    tf_text = {word: count / len(tokens) for word, count in tf_text.items()}\n",
    "    return tf_text\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    X_train = X[train_indices]\n",
    "    X_val = X[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_val = y.iloc[test_indices]\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "\n",
    "# tfidf_matrix = []\n",
    "# for text in corpus:\n",
    "#     tfidf_text = {}\n",
    "#     computed_tf = compute_tf(text)\n",
    "#     for word in computed_tf:\n",
    "#         tfidf_text[word] = computed_tf[word] * idf_dict.get(word, 0)\n",
    "#     tfidf_matrix.append(tfidf_text)\n",
    "\n",
    "# # Convert the tfidf_matrix to a consistent format\n",
    "# unique_words = set(word for doc in tfidf_matrix for word in doc.keys())\n",
    "# word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "# tfidf_vectors = []\n",
    "# for doc in tfidf_matrix:\n",
    "#     vector = np.zeros(len(unique_words))\n",
    "#     for word, tfidf in doc.items():\n",
    "#         vector[word_index[word]] = tfidf\n",
    "#     tfidf_vectors.append(vector)\n",
    "\n",
    "# tfidf_vectors = np.array(tfidf_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn_model:\n",
    "    def __init__(self,k=5):\n",
    "        self.k = k\n",
    "        \n",
    "    def euclidean_distance(self,a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        # Compute TF-IDF\n",
    "        \n",
    "        idf_dict = compute_idf(corpus)\n",
    "        tfidf_matrix = []\n",
    "        for text in corpus:\n",
    "            tfidf_text = {}\n",
    "            computed_tf = compute_tf(text)\n",
    "            for word in computed_tf:\n",
    "                tfidf_text[word] = computed_tf[word] * idf_dict.get(word, 0)\n",
    "            tfidf_matrix.append(tfidf_text)\n",
    "\n",
    "        # Convert the tfidf_matrix to a consistent format\n",
    "        unique_words = set(word for doc in tfidf_matrix for word in doc.keys())\n",
    "        \n",
    "        word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "        tfidf_vectors = []\n",
    "        for doc in tfidf_matrix:\n",
    "            vector = np.zeros(len(unique_words))\n",
    "            for word, tfidf in doc.items():\n",
    "                vector[word_index[word]] = tfidf\n",
    "            tfidf_vectors.append(vector)\n",
    "\n",
    "        self.tfidf_vectors = np.array(tfidf_vectors)\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.tfidf_vectors, data['genre'], test_size=0.3, random_state=42)\n",
    "        return self.tfidf_vectors\n",
    "\n",
    "    # KNN predict function\n",
    "    def predict(self, x_test):\n",
    "        distances = []\n",
    "        for i in range(len(X_train)):\n",
    "            dist = self.euclidean_distance(self.X_train[i], x_test)\n",
    "            distances.append((dist, self.y_train.iloc[i]))\n",
    "        \n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        neighbors = distances[:self.k]\n",
    "        output_values = [neighbor[1] for neighbor in neighbors]\n",
    "        prediction = Counter(output_values).most_common(1)[0]\n",
    "        return prediction\n",
    "\n",
    "corpus = data['description'].tolist()\n",
    "k = knn_model()\n",
    "tfidf_vectors = k.fit(corpus)       \n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf_vectors, data['genre'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score  support\n",
      "crime         0.000000  0.000000  0.000000      0.0\n",
      "western       0.000000  0.000000  0.000000      0.0\n",
      "sci-fi        0.000000  0.000000  0.000000      0.0\n",
      "animation     0.000000  0.000000  0.000000      0.0\n",
      "comedy        0.250000  0.090909  0.133333      4.0\n",
      "documentary   0.311828  0.966667  0.471545     93.0\n",
      "drama         0.000000  0.000000  0.000000      2.0\n",
      "action        0.000000  0.000000  0.000000      0.0\n",
      "horror        0.000000  0.000000  0.000000      0.0\n",
      "mystery       0.000000  0.000000  0.000000      0.0\n",
      "family        0.000000  0.000000  0.000000      0.0\n",
      "adventure     0.000000  0.000000  0.000000      0.0\n",
      "reality-tv    0.000000  0.000000  0.000000      0.0\n",
      "romance       0.000000  0.000000  0.000000      0.0\n",
      "short         0.000000  0.000000  0.000000      1.0\n",
      "thriller      0.000000  0.000000  0.000000      0.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = [k.predict(x)[0] for x in X_val]\n",
    "\n",
    "# Classification report function\n",
    "def classification_report(y_true, y_pred):\n",
    "    unique_genres = {x:0 for x in list(data['genre'].unique())}\n",
    "    labels = list(set(y_true))\n",
    "    y_pred_list = list(y_pred)\n",
    "    \n",
    "    for x in y_pred_list:\n",
    "        unique_genres[x]+=1\n",
    "    \n",
    "    report = {}\n",
    "    for label in labels:\n",
    "        \n",
    "        true_positive = sum((y_true == label) & (y_pred == label))\n",
    "        false_positive = sum((y_true != label) & (y_pred == label))\n",
    "        false_negative = sum((y_true == label) & (y_pred != label))\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        report[label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1-score': f1_score,\n",
    "            'support': unique_genres[label]\n",
    "            \n",
    "        }\n",
    "        report = pd.DataFrame(report)\n",
    "    \n",
    "    return report.T\n",
    "\n",
    "print(classification_report(np.array(y_val), np.array(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ذخیره مدل و بردار ساز برای استفاده در آینده\n",
    "import joblib\n",
    "joblib.dump(k, 'knn_model.pkl')\n",
    "joblib.dump(tfidf_vectors, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Genre: documentary\n",
      "comedy\n"
     ]
    }
   ],
   "source": [
    "# بارگذاری مدل و بردار ساز\n",
    "model = joblib.load('knn_model.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Test prediction\n",
    "model.fit(corpus)\n",
    "\n",
    "# Preprocess and vectorize function\n",
    "def vectorizer_transform(description):\n",
    "    description = preprocess_text(description[0])\n",
    "    computed_tf = compute_tf(description)\n",
    "    tfidf_vector = np.zeros(len(unique_words))\n",
    "    for word in description.split():\n",
    "        if word in word_index:\n",
    "            tfidf_vector[word_index[word]] = computed_tf[word] * idf_dict.get(word, 0)\n",
    "    return tfidf_vector\n",
    "\n",
    "# Predict genre function\n",
    "def predict_genre(description):\n",
    "    description_vector = vectorizer_transform([description])\n",
    "    predicted_genre = model.predict( description_vector)\n",
    "    return predicted_genre[0]\n",
    "\n",
    "\n",
    "\n",
    "index = 86\n",
    "new_description = test_data['description'][index]\n",
    "predicted_genre = predict_genre(new_description)\n",
    "print(f\"Predicted Genre: {predicted_genre}\")\n",
    "print(test_data['genre'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentary</th>\n",
       "      <th>animation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.280303</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.432749</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>78.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           documentary  animation\n",
       "precision     0.280303        0.0\n",
       "recall        0.948718        0.0\n",
       "f1-score      0.432749        0.0\n",
       "support      78.000000        3.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'documentary': {'precision': 0.2803030303030303, 'recall': 0.9487179487179487, 'f1-score': 0.4327485380116959, 'support': 78}, 'animation': {'precision': 0, 'recall': 0.0, 'f1-score': 0, 'support': 3}}\n",
    "df= pd.DataFrame(d)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qasem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qasem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Qasem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Qasem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import copy\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.txt', delimiter=' ::: ', engine='python')\n",
    "test_data = pd.read_csv('test.txt', delimiter=' ::: ', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing train data and\n",
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Preprocess data\n",
    "\n",
    "preprocess_data = copy.deepcopy(train_data.head(1000))\n",
    "preprocess_data['description'] = preprocess_data['description'].apply(preprocess_text)\n",
    "data = preprocess_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# X_train = copy.deepcopy(data['description'])\n",
    "# y_train = copy.deepcopy(data['genre'])\n",
    "\n",
    "# # تبدیل داده‌ها به فرمت مناسب\n",
    "# X_train = X_train.values.reshape(-1, 1)\n",
    "\n",
    "# # ایجاد نمونه‌های کاهشی\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# # تبدیل داده‌های کاهشی به DataFrame\n",
    "# resampled_train_data = pd.DataFrame({\n",
    "#     'description': X_resampled.flatten(),\n",
    "#     'genre': y_resampled\n",
    "# })\n",
    "\n",
    "# des = []\n",
    "# genre = []\n",
    "# for i in range(len(X_resampled)):\n",
    "#     des.append(X_resampled[i][0])\n",
    "#     genre.append(y_resampled[i][0])\n",
    "\n",
    "# n = pd.DataFrame({'description':des, 'genre':y_resampled})\n",
    "# data = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Precompute IDF values\n",
    "def compute_idf(corpus):\n",
    "    idf_dict = defaultdict(int)\n",
    "    for document in corpus:\n",
    "        unique_words = set(document.split())\n",
    "        for word in unique_words:\n",
    "            idf_dict[word] += 1\n",
    "    total_documents = len(corpus)\n",
    "    idf_dict = {word: np.log10(total_documents / count) for word, count in idf_dict.items()}\n",
    "    return idf_dict\n",
    "\n",
    "# Compute TF-IDF\n",
    "corpus = data['description'].tolist()\n",
    "\n",
    "idf_dict = compute_idf(corpus)\n",
    "\n",
    "def compute_tf(text):\n",
    "    tokens = text.split()\n",
    "    tf_text = Counter(tokens)\n",
    "    tf_text = {word: count / len(tokens) for word, count in tf_text.items()}\n",
    "    return tf_text\n",
    "\n",
    "# tfidf_matrix = []\n",
    "# for text in corpus:\n",
    "#     tfidf_text = {}\n",
    "#     computed_tf = compute_tf(text)\n",
    "#     for word in computed_tf:\n",
    "#         tfidf_text[word] = computed_tf[word] * idf_dict.get(word, 0)\n",
    "#     tfidf_matrix.append(tfidf_text)\n",
    "\n",
    "# # Convert the tfidf_matrix to a consistent format\n",
    "# unique_words = set(word for doc in tfidf_matrix for word in doc.keys())\n",
    "# word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "# tfidf_vectors = []\n",
    "# for doc in tfidf_matrix:\n",
    "#     vector = np.zeros(len(unique_words))\n",
    "#     for word, tfidf in doc.items():\n",
    "#         vector[word_index[word]] = tfidf\n",
    "#     tfidf_vectors.append(vector)\n",
    "\n",
    "# tfidf_vectors = np.array(tfidf_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn_model:\n",
    "\n",
    "    def euclidean_distance(self,a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    def fit(self):\n",
    "        tfidf_matrix = []\n",
    "        for text in corpus:\n",
    "            tfidf_text = {}\n",
    "            computed_tf = compute_tf(text)\n",
    "            for word in computed_tf:\n",
    "                tfidf_text[word] = computed_tf[word] * idf_dict.get(word, 0)\n",
    "            tfidf_matrix.append(tfidf_text)\n",
    "\n",
    "        # Convert the tfidf_matrix to a consistent format\n",
    "        unique_words = set(word for doc in tfidf_matrix for word in doc.keys())\n",
    "        word_index = {word: i for i, word in enumerate(unique_words)}\n",
    "        tfidf_vectors = []\n",
    "        for doc in tfidf_matrix:\n",
    "            vector = np.zeros(len(unique_words))\n",
    "            for word, tfidf in doc.items():\n",
    "                vector[word_index[word]] = tfidf\n",
    "            tfidf_vectors.append(vector)\n",
    "\n",
    "        self.tfidf_vectors = np.array(tfidf_vectors)\n",
    "\n",
    "    # KNN predict function\n",
    "    def knn_predict(self, x_test, k=5):\n",
    "        distances = []\n",
    "        for i in range(len(X_train)):\n",
    "            dist = self.euclidean_distance(X_train[i], x_test)\n",
    "            distances.append((dist, y_train.iloc[i]))\n",
    "        \n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        neighbors = distances[:k]\n",
    "        output_values = [neighbor[1] for neighbor in neighbors]\n",
    "        prediction = Counter(output_values).most_common(1)[0][0]\n",
    "        return prediction\n",
    "\n",
    "\n",
    "k = knn_model()\n",
    "tfidf_vectors = k.fit()       \n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf_vectors, data['genre'], test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    X_train = X[train_indices]\n",
    "    X_val = X[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_val = y.iloc[test_indices]\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# y_pred = [k.knn_predict(X_train, y_train, x) for x in X_val]\n",
    "# joblib.dump(k)\n",
    "# # Classification report function\n",
    "# def classification_report(y_true, y_pred):\n",
    "#     labels = list(set(y_true))\n",
    "#     report = {}\n",
    "#     for label in labels:\n",
    "#         true_positive = sum((y_true == label) & (y_pred == label))\n",
    "#         false_positive = sum((y_true != label) & (y_pred == label))\n",
    "#         false_negative = sum((y_true == label) & (y_pred != label))\n",
    "#         precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "#         recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "#         f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "#         report[label] = {\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1-score': f1_score\n",
    "#         }\n",
    "#     return report\n",
    "\n",
    "# print(classification_report(np.array(y_val), np.array(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5774\u001b[39m\n\u001b[0;32m     20\u001b[0m new_description \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\n\u001b[1;32m---> 21\u001b[0m predicted_genre \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_genre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Genre: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_genre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m][index])\n",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m, in \u001b[0;36mpredict_genre\u001b[1;34m(description, X_train, y_train, corpus)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_genre\u001b[39m(description, X_train, y_train, corpus):\n\u001b[0;32m     13\u001b[0m     description_vector \u001b[38;5;241m=\u001b[39m preprocess_and_vectorize(description, corpus)\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     predicted_genre \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mknn_predict( description_vector)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_genre\n",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m, in \u001b[0;36mknn_model.fit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m      7\u001b[0m distances \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[1;32m----> 9\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meuclidean_distance(X_train[i], \u001b[43mx_test\u001b[49m)\n\u001b[0;32m     10\u001b[0m     distances\u001b[38;5;241m.\u001b[39mappend((dist, y_train\u001b[38;5;241m.\u001b[39miloc[i]))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistances \u001b[38;5;241m=\u001b[39m distances\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess and vectorize function\n",
    "def preprocess_and_vectorize(description, corpus):\n",
    "    description = preprocess_text(description)\n",
    "    computed_tf = compute_tf(description)\n",
    "    tfidf_vector = np.zeros(len(unique_words))\n",
    "    for word in description.split():\n",
    "        if word in word_index:\n",
    "            tfidf_vector[word_index[word]] = computed_tf[word] * idf_dict.get(word, 0)\n",
    "    return tfidf_vector\n",
    "\n",
    "# Predict genre function\n",
    "def predict_genre(description, X_train, y_train, corpus):\n",
    "    description_vector = preprocess_and_vectorize(description, corpus)\n",
    "    k.fit(X_train, y_train)\n",
    "    predicted_genre = k.knn_predict( description_vector)\n",
    "    return predicted_genre\n",
    "\n",
    "# Test prediction\n",
    "index = 5774\n",
    "new_description = test_data['description'][index]\n",
    "predicted_genre = predict_genre(new_description, X_train, y_train, corpus)\n",
    "print(f\"Predicted Genre: {predicted_genre}\")\n",
    "print(test_data['genre'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
